{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Importing required libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import torch.nn as nn\n",
    "import torchmetrics\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Reading in the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ML_data2.csv\",header=None)\n",
    "data\n",
    "data = data.to_numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting the device for torch"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Splitting the data into train (50%), test (25%), and validation (25%)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(data[:,:-1], data[:,-1],\n",
    "                                                                                  train_size=0.5, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size=0.5, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For classification, converting the values in the amount overspent into 0 if it is underspent or 1 if it is overspent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "y_train = np.where(y_train<=0,0,1)\n",
    "y_val = np.where(y_val<=0,0,1)\n",
    "y_test = np.where(y_test<=0,0,1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generating torch datasets from the data to be able to easily batch them"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "X_train_tensor = TensorDataset(torch.Tensor(X_train),torch.Tensor(y_train))\n",
    "X_val_tensor = TensorDataset(torch.Tensor(X_val),torch.Tensor(y_val))\n",
    "X_test_tensor = TensorDataset(torch.Tensor(X_test),torch.Tensor(y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generating the dataloaders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# creating dataloaders for the datasets\n",
    "\n",
    "# arguments for the dataloaders\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "batch_size = 100\n",
    "# dataloaders\n",
    "train_loader = DataLoader(dataset=X_train_tensor, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "valid_loader = DataLoader(dataset=X_val_tensor, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "test_loader  = DataLoader(dataset=X_test_tensor ,  batch_size=batch_size, shuffle=False,  **kwargs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing dataloaders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 4]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(batch_x.shape, batch_y.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Neural Network class with 3 dense layers, and a sigmoid activation layer at the end to get values between 0-1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.linear3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = torch.sigmoid(self.linear3(x))\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "hidden_dim_1 = 64\n",
    "hidden_dim_2 = 64\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "network = Network(4,hidden_dim_1,hidden_dim_2,1)\n",
    "\n",
    "loss_function = F.binary_cross_entropy\n",
    "optimizer = Adam(network.parameters(), lr=lr)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training Loop"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 2  train\tAverage Loss:  0.007502219591821943\n",
      "\tEpoch 2  valid\tAverage Loss:  0.007337974835844601\n",
      "\tEpoch 3  train\tAverage Loss:  0.007460681055273329\n",
      "\tEpoch 3  valid\tAverage Loss:  0.007630708918851965\n",
      "\tEpoch 4  train\tAverage Loss:  0.011125243033681597\n",
      "\tEpoch 4  valid\tAverage Loss:  0.006658045053482056\n",
      "\tEpoch 5  train\tAverage Loss:  0.010453290368829455\n",
      "\tEpoch 5  valid\tAverage Loss:  0.00801768702619216\n",
      "\tEpoch 6  train\tAverage Loss:  0.00886070201226643\n",
      "\tEpoch 6  valid\tAverage Loss:  0.0067816795671687405\n",
      "\tEpoch 7  train\tAverage Loss:  0.009930590569972992\n",
      "\tEpoch 7  valid\tAverage Loss:  0.012839403853696935\n",
      "\tEpoch 8  train\tAverage Loss:  0.016574992162840706\n",
      "\tEpoch 8  valid\tAverage Loss:  0.016105057632221895\n",
      "\tEpoch 9  train\tAverage Loss:  0.009384420667375838\n",
      "\tEpoch 9  valid\tAverage Loss:  0.009288057790083043\n",
      "\tEpoch 10  train\tAverage Loss:  0.0079748290010861\n",
      "\tEpoch 10  valid\tAverage Loss:  0.007082099704181446\n",
      "\tEpoch 11  train\tAverage Loss:  0.0067494869572775704\n",
      "\tEpoch 11  valid\tAverage Loss:  0.00681507689111373\n",
      "\tEpoch 12  train\tAverage Loss:  0.008209673719746726\n",
      "\tEpoch 12  valid\tAverage Loss:  0.006604888474240023\n",
      "\tEpoch 13  train\tAverage Loss:  0.00849242947782789\n",
      "\tEpoch 13  valid\tAverage Loss:  0.010089839872191935\n",
      "\tEpoch 14  train\tAverage Loss:  0.009818853769983564\n",
      "\tEpoch 14  valid\tAverage Loss:  0.01023383799721213\n",
      "\tEpoch 15  train\tAverage Loss:  0.009508987775870732\n",
      "\tEpoch 15  valid\tAverage Loss:  0.01777525123427896\n",
      "\tEpoch 16  train\tAverage Loss:  0.013131653700556073\n",
      "\tEpoch 16  valid\tAverage Loss:  0.014224305012646843\n",
      "\tEpoch 17  train\tAverage Loss:  0.007210433755602155\n",
      "\tEpoch 17  valid\tAverage Loss:  0.006979068728054271\n",
      "\tEpoch 18  train\tAverage Loss:  0.008266124188899994\n",
      "\tEpoch 18  valid\tAverage Loss:  0.00970607249175801\n",
      "\tEpoch 19  train\tAverage Loss:  0.007862209115709578\n",
      "\tEpoch 19  valid\tAverage Loss:  0.010137173463316525\n",
      "\tEpoch 20  train\tAverage Loss:  0.009132343573229654\n",
      "\tEpoch 20  valid\tAverage Loss:  0.016230538838049945\n",
      "\tEpoch 21  train\tAverage Loss:  0.007718616911343166\n",
      "\tEpoch 21  valid\tAverage Loss:  0.007282511591911316\n",
      "\tEpoch 22  train\tAverage Loss:  0.007293588323252542\n",
      "\tEpoch 22  valid\tAverage Loss:  0.006782706414951998\n",
      "\tEpoch 23  train\tAverage Loss:  0.007412936202117375\n",
      "\tEpoch 23  valid\tAverage Loss:  0.009357723839142743\n",
      "\tEpoch 24  train\tAverage Loss:  0.008514360896178654\n",
      "\tEpoch 24  valid\tAverage Loss:  0.007269193495021147\n",
      "\tEpoch 25  train\tAverage Loss:  0.006507261838231768\n",
      "\tEpoch 25  valid\tAverage Loss:  0.00812410105677212\n",
      "\tEpoch 26  train\tAverage Loss:  0.007677917727402278\n",
      "\tEpoch 26  valid\tAverage Loss:  0.008720853889689725\n",
      "\tEpoch 27  train\tAverage Loss:  0.008291048722607748\n",
      "\tEpoch 27  valid\tAverage Loss:  0.009734186705420998\n",
      "\tEpoch 28  train\tAverage Loss:  0.00690878690140588\n",
      "\tEpoch 28  valid\tAverage Loss:  0.007793379320817835\n",
      "\tEpoch 29  train\tAverage Loss:  0.007168670415878296\n",
      "\tEpoch 29  valid\tAverage Loss:  0.007016853129162508\n",
      "\tEpoch 30  train\tAverage Loss:  0.007596119906221117\n",
      "\tEpoch 30  valid\tAverage Loss:  0.010462528046439675\n",
      "\tEpoch 31  train\tAverage Loss:  0.007587012103625706\n",
      "\tEpoch 31  valid\tAverage Loss:  0.0073251590308021095\n",
      "\tEpoch 32  train\tAverage Loss:  0.0069327256424086435\n",
      "\tEpoch 32  valid\tAverage Loss:  0.008401236989918877\n",
      "\tEpoch 33  train\tAverage Loss:  0.0069189137475831166\n",
      "\tEpoch 33  valid\tAverage Loss:  0.00860042063628926\n",
      "\tEpoch 34  train\tAverage Loss:  0.007053151999201093\n",
      "\tEpoch 34  valid\tAverage Loss:  0.008744970840566298\n",
      "\tEpoch 35  train\tAverage Loss:  0.007395807615348271\n",
      "\tEpoch 35  valid\tAverage Loss:  0.00664498318644131\n",
      "\tEpoch 36  train\tAverage Loss:  0.007094593175819942\n",
      "\tEpoch 36  valid\tAverage Loss:  0.0071156333123936375\n",
      "\tEpoch 37  train\tAverage Loss:  0.007603275835514069\n",
      "\tEpoch 37  valid\tAverage Loss:  0.011148678905823651\n",
      "\tEpoch 38  train\tAverage Loss:  0.00775134665625436\n",
      "\tEpoch 38  valid\tAverage Loss:  0.007000936059390797\n",
      "\tEpoch 39  train\tAverage Loss:  0.007175537696906499\n",
      "\tEpoch 39  valid\tAverage Loss:  0.00682135424193214\n",
      "\tEpoch 40  train\tAverage Loss:  0.006985626297337668\n",
      "\tEpoch 40  valid\tAverage Loss:  0.006923054561895483\n",
      "\tEpoch 41  train\tAverage Loss:  0.006711928376129695\n",
      "\tEpoch 41  valid\tAverage Loss:  0.008789889531977036\n",
      "\tEpoch 42  train\tAverage Loss:  0.0068960162656647814\n",
      "\tEpoch 42  valid\tAverage Loss:  0.008194521350019118\n",
      "\tEpoch 43  train\tAverage Loss:  0.007414895227977208\n",
      "\tEpoch 43  valid\tAverage Loss:  0.0066003501415252686\n",
      "\tEpoch 44  train\tAverage Loss:  0.007066904519285475\n",
      "\tEpoch 44  valid\tAverage Loss:  0.006998757614808924\n",
      "\tEpoch 45  train\tAverage Loss:  0.007564063165869031\n",
      "\tEpoch 45  valid\tAverage Loss:  0.011415178355048685\n",
      "\tEpoch 46  train\tAverage Loss:  0.007694814937455313\n",
      "\tEpoch 46  valid\tAverage Loss:  0.007013975206543417\n",
      "\tEpoch 47  train\tAverage Loss:  0.007041955811636788\n",
      "\tEpoch 47  valid\tAverage Loss:  0.0069191762629677266\n",
      "\tEpoch 48  train\tAverage Loss:  0.007491055812154497\n",
      "\tEpoch 48  valid\tAverage Loss:  0.010644769984133103\n",
      "\tEpoch 49  train\tAverage Loss:  0.007584113052913121\n",
      "\tEpoch 49  valid\tAverage Loss:  0.006623463560553158\n",
      "\tEpoch 50  train\tAverage Loss:  0.007387106622968401\n",
      "\tEpoch 50  valid\tAverage Loss:  0.01118573174757116\n",
      "\tEpoch 51  train\tAverage Loss:  0.007203421064785548\n",
      "\tEpoch 51  valid\tAverage Loss:  0.007990430873983047\n",
      "\tEpoch 52  train\tAverage Loss:  0.006892211581979479\n",
      "\tEpoch 52  valid\tAverage Loss:  0.0069514409584157605\n",
      "\tEpoch 53  train\tAverage Loss:  0.0074051163792610165\n",
      "\tEpoch 53  valid\tAverage Loss:  0.011137635497485891\n",
      "\tEpoch 54  train\tAverage Loss:  0.007186592033931187\n",
      "\tEpoch 54  valid\tAverage Loss:  0.007164500986828523\n",
      "\tEpoch 55  train\tAverage Loss:  0.00735981479712895\n",
      "\tEpoch 55  valid\tAverage Loss:  0.010888937080607695\n",
      "\tEpoch 56  train\tAverage Loss:  0.0071343854495457245\n",
      "\tEpoch 56  valid\tAverage Loss:  0.007940634874736561\n",
      "\tEpoch 57  train\tAverage Loss:  0.006812224337032863\n",
      "\tEpoch 57  valid\tAverage Loss:  0.0072061953123878034\n",
      "\tEpoch 58  train\tAverage Loss:  0.007100868667875017\n",
      "\tEpoch 58  valid\tAverage Loss:  0.006652013063430786\n",
      "\tEpoch 59  train\tAverage Loss:  0.008084963662283761\n",
      "\tEpoch 59  valid\tAverage Loss:  0.012149001430062686\n",
      "\tEpoch 60  train\tAverage Loss:  0.020783702066966465\n",
      "\tEpoch 60  valid\tAverage Loss:  0.010575033566531012\n",
      "\tEpoch 61  train\tAverage Loss:  0.010820694846766336\n",
      "\tEpoch 61  valid\tAverage Loss:  0.013802353318999795\n",
      "\tEpoch 62  train\tAverage Loss:  0.00855333069392613\n",
      "\tEpoch 62  valid\tAverage Loss:  0.008891928055707146\n",
      "\tEpoch 63  train\tAverage Loss:  0.007348648139408657\n",
      "\tEpoch 63  valid\tAverage Loss:  0.008690226568895228\n",
      "\tEpoch 64  train\tAverage Loss:  0.007510638271059309\n",
      "\tEpoch 64  valid\tAverage Loss:  0.006609936426667606\n",
      "\tEpoch 65  train\tAverage Loss:  0.007148730933666229\n",
      "\tEpoch 65  valid\tAverage Loss:  0.008500267162042505\n",
      "\tEpoch 66  train\tAverage Loss:  0.006923971269811903\n",
      "\tEpoch 66  valid\tAverage Loss:  0.007160844136686886\n",
      "\tEpoch 67  train\tAverage Loss:  0.0067149080463818145\n",
      "\tEpoch 67  valid\tAverage Loss:  0.007827193526660696\n",
      "\tEpoch 68  train\tAverage Loss:  0.007507582630429949\n",
      "\tEpoch 68  valid\tAverage Loss:  0.0065978894163580505\n",
      "\tEpoch 69  train\tAverage Loss:  0.007229341907160623\n",
      "\tEpoch 69  valid\tAverage Loss:  0.008907205392332638\n",
      "\tEpoch 70  train\tAverage Loss:  0.0065978872690882\n",
      "\tEpoch 70  valid\tAverage Loss:  0.007398714037502513\n",
      "\tEpoch 71  train\tAverage Loss:  0.006949324003287724\n",
      "\tEpoch 71  valid\tAverage Loss:  0.009140545234960669\n",
      "\tEpoch 72  train\tAverage Loss:  0.007540668777057103\n",
      "\tEpoch 72  valid\tAverage Loss:  0.006606503199128543\n",
      "\tEpoch 73  train\tAverage Loss:  0.0071872551441192626\n",
      "\tEpoch 73  valid\tAverage Loss:  0.008720005294855903\n",
      "\tEpoch 74  train\tAverage Loss:  0.006845968774386814\n",
      "\tEpoch 74  valid\tAverage Loss:  0.007998944801442764\n",
      "\tEpoch 75  train\tAverage Loss:  0.0073052620462008885\n",
      "\tEpoch 75  valid\tAverage Loss:  0.006768703706124249\n",
      "\tEpoch 76  train\tAverage Loss:  0.00696663578919002\n",
      "\tEpoch 76  valid\tAverage Loss:  0.009164145483690149\n",
      "\tEpoch 77  train\tAverage Loss:  0.006742519642625536\n",
      "\tEpoch 77  valid\tAverage Loss:  0.006605503699358772\n",
      "\tEpoch 78  train\tAverage Loss:  0.006779290216309684\n",
      "\tEpoch 78  valid\tAverage Loss:  0.0088301319585127\n",
      "\tEpoch 79  train\tAverage Loss:  0.007457573652267456\n",
      "\tEpoch 79  valid\tAverage Loss:  0.006615711766130784\n",
      "\tEpoch 80  train\tAverage Loss:  0.0069808187229292735\n",
      "\tEpoch 80  valid\tAverage Loss:  0.00891774521154516\n",
      "\tEpoch 81  train\tAverage Loss:  0.006822692539010729\n",
      "\tEpoch 81  valid\tAverage Loss:  0.007093814330942491\n",
      "\tEpoch 82  train\tAverage Loss:  0.006628063602106912\n",
      "\tEpoch 82  valid\tAverage Loss:  0.007660056282492245\n",
      "\tEpoch 83  train\tAverage Loss:  0.007297711185046605\n",
      "\tEpoch 83  valid\tAverage Loss:  0.006609442409347085\n",
      "\tEpoch 84  train\tAverage Loss:  0.0069632807970047\n",
      "\tEpoch 84  valid\tAverage Loss:  0.008899887659970452\n",
      "\tEpoch 85  train\tAverage Loss:  0.0067470638751983645\n",
      "\tEpoch 85  valid\tAverage Loss:  0.006696898498955895\n",
      "\tEpoch 86  train\tAverage Loss:  0.006766083104269845\n",
      "\tEpoch 86  valid\tAverage Loss:  0.008549828950096578\n",
      "\tEpoch 87  train\tAverage Loss:  0.00735533367735999\n",
      "\tEpoch 87  valid\tAverage Loss:  0.006618200610665714\n",
      "\tEpoch 88  train\tAverage Loss:  0.006987163186073303\n",
      "\tEpoch 88  valid\tAverage Loss:  0.008447497241637287\n",
      "\tEpoch 89  train\tAverage Loss:  0.006773240225655692\n",
      "\tEpoch 89  valid\tAverage Loss:  0.007507589810034808\n",
      "\tEpoch 90  train\tAverage Loss:  0.007248247717108045\n",
      "\tEpoch 90  valid\tAverage Loss:  0.006940130521269405\n",
      "\tEpoch 91  train\tAverage Loss:  0.006719627107892718\n",
      "\tEpoch 91  valid\tAverage Loss:  0.008650068710832034\n",
      "\tEpoch 92  train\tAverage Loss:  0.007286375428949083\n",
      "\tEpoch 92  valid\tAverage Loss:  0.00661044509971843\n",
      "\tEpoch 93  train\tAverage Loss:  0.006816360924925123\n",
      "\tEpoch 93  valid\tAverage Loss:  0.008699827474706313\n",
      "\tEpoch 94  train\tAverage Loss:  0.006539814863886152\n",
      "\tEpoch 94  valid\tAverage Loss:  0.006639084815979004\n",
      "\tEpoch 95  train\tAverage Loss:  0.006422668252672468\n",
      "\tEpoch 95  valid\tAverage Loss:  0.006630553497987635\n",
      "\tEpoch 96  train\tAverage Loss:  0.0067085980602673125\n",
      "\tEpoch 96  valid\tAverage Loss:  0.00830882843802957\n",
      "\tEpoch 97  train\tAverage Loss:  0.007239601331097739\n",
      "\tEpoch 97  valid\tAverage Loss:  0.006610908788793227\n",
      "\tEpoch 98  train\tAverage Loss:  0.006795462480613163\n",
      "\tEpoch 98  valid\tAverage Loss:  0.008592079281806947\n",
      "\tEpoch 99  train\tAverage Loss:  0.00646735348871776\n",
      "\tEpoch 99  valid\tAverage Loss:  0.007071951312177321\n",
      "\tEpoch 100  train\tAverage Loss:  0.006423968425818852\n",
      "\tEpoch 100  valid\tAverage Loss:  0.00663470934419071\n",
      "\tEpoch 101  train\tAverage Loss:  0.006641226155417306\n",
      "\tEpoch 101  valid\tAverage Loss:  0.008714421005810009\n",
      "\tEpoch 102  train\tAverage Loss:  0.007287935129233769\n",
      "\tEpoch 102  valid\tAverage Loss:  0.00661191554630504\n",
      "\tEpoch 103  train\tAverage Loss:  0.006675048870699746\n",
      "\tEpoch 103  valid\tAverage Loss:  0.008171283217037426\n",
      "\tEpoch 104  train\tAverage Loss:  0.007170968123844692\n",
      "\tEpoch 104  valid\tAverage Loss:  0.006618519144899705\n",
      "\tEpoch 105  train\tAverage Loss:  0.006698874379907335\n",
      "\tEpoch 105  valid\tAverage Loss:  0.008032979263978847\n",
      "\tEpoch 106  train\tAverage Loss:  0.00709575743334634\n",
      "\tEpoch 106  valid\tAverage Loss:  0.00720729880473193\n",
      "\tEpoch 107  train\tAverage Loss:  0.00639442058120455\n",
      "\tEpoch 107  valid\tAverage Loss:  0.007088066023938796\n",
      "\tEpoch 108  train\tAverage Loss:  0.006376316700662885\n",
      "\tEpoch 108  valid\tAverage Loss:  0.006641545751515557\n",
      "\tEpoch 109  train\tAverage Loss:  0.006603196859359741\n",
      "\tEpoch 109  valid\tAverage Loss:  0.008481096940882065\n",
      "\tEpoch 110  train\tAverage Loss:  0.0071857607705252515\n",
      "\tEpoch 110  valid\tAverage Loss:  0.006622798968763912\n",
      "\tEpoch 111  train\tAverage Loss:  0.006632165840693882\n",
      "\tEpoch 111  valid\tAverage Loss:  0.008093872701420503\n",
      "\tEpoch 112  train\tAverage Loss:  0.00707547185250691\n",
      "\tEpoch 112  valid\tAverage Loss:  0.006702516780180089\n",
      "\tEpoch 113  train\tAverage Loss:  0.006571817125592913\n",
      "\tEpoch 113  valid\tAverage Loss:  0.008390141234678381\n",
      "\tEpoch 114  train\tAverage Loss:  0.00707039122070585\n",
      "\tEpoch 114  valid\tAverage Loss:  0.006706294540096732\n",
      "\tEpoch 115  train\tAverage Loss:  0.006531938007899693\n",
      "\tEpoch 115  valid\tAverage Loss:  0.008309358744060292\n",
      "\tEpoch 116  train\tAverage Loss:  0.00704401900938579\n",
      "\tEpoch 116  valid\tAverage Loss:  0.006737158386146321\n",
      "\tEpoch 117  train\tAverage Loss:  0.006461617529392242\n",
      "\tEpoch 117  valid\tAverage Loss:  0.007754765889223884\n",
      "\tEpoch 118  train\tAverage Loss:  0.006988448926380703\n",
      "\tEpoch 118  valid\tAverage Loss:  0.0066508248448371885\n",
      "\tEpoch 119  train\tAverage Loss:  0.006517710932663508\n",
      "\tEpoch 119  valid\tAverage Loss:  0.00822579653824077\n",
      "\tEpoch 120  train\tAverage Loss:  0.0069962592976433894\n",
      "\tEpoch 120  valid\tAverage Loss:  0.0070710328747244445\n",
      "\tEpoch 121  train\tAverage Loss:  0.006385766540254865\n",
      "\tEpoch 121  valid\tAverage Loss:  0.006645213330493254\n",
      "\tEpoch 122  train\tAverage Loss:  0.006238869360515049\n",
      "\tEpoch 122  valid\tAverage Loss:  0.006626138617010678\n",
      "\tEpoch 123  train\tAverage Loss:  0.006477367103099823\n",
      "\tEpoch 123  valid\tAverage Loss:  0.008295959584853229\n",
      "\tEpoch 124  train\tAverage Loss:  0.006993567705154419\n",
      "\tEpoch 124  valid\tAverage Loss:  0.006829866013106178\n",
      "\tEpoch 125  train\tAverage Loss:  0.006330611833504268\n",
      "\tEpoch 125  valid\tAverage Loss:  0.006670315546147963\n",
      "\tEpoch 126  train\tAverage Loss:  0.006275864677769797\n",
      "\tEpoch 126  valid\tAverage Loss:  0.006672352622537052\n",
      "\tEpoch 127  train\tAverage Loss:  0.006487904386860983\n",
      "\tEpoch 127  valid\tAverage Loss:  0.008297310857211842\n",
      "\tEpoch 128  train\tAverage Loss:  0.007000304426465716\n",
      "\tEpoch 128  valid\tAverage Loss:  0.0068453833636115576\n",
      "\tEpoch 129  train\tAverage Loss:  0.006314921336514609\n",
      "\tEpoch 129  valid\tAverage Loss:  0.006640965377583223\n",
      "\tEpoch 130  train\tAverage Loss:  0.006280973672866821\n",
      "\tEpoch 130  valid\tAverage Loss:  0.006717003724154304\n",
      "\tEpoch 131  train\tAverage Loss:  0.006449925524847848\n",
      "\tEpoch 131  valid\tAverage Loss:  0.007967331689946792\n",
      "\tEpoch 132  train\tAverage Loss:  0.006985896757670812\n",
      "\tEpoch 132  valid\tAverage Loss:  0.006706524596494787\n",
      "\tEpoch 133  train\tAverage Loss:  0.0064306510175977435\n",
      "\tEpoch 133  valid\tAverage Loss:  0.007894461540614857\n",
      "\tEpoch 134  train\tAverage Loss:  0.006953545664037977\n",
      "\tEpoch 134  valid\tAverage Loss:  0.006854708545348223\n",
      "\tEpoch 135  train\tAverage Loss:  0.006292330358709608\n",
      "\tEpoch 135  valid\tAverage Loss:  0.006642898461397957\n",
      "\tEpoch 136  train\tAverage Loss:  0.006222453364304134\n",
      "\tEpoch 136  valid\tAverage Loss:  0.0066333212221370026\n",
      "\tEpoch 137  train\tAverage Loss:  0.006447550049849919\n",
      "\tEpoch 137  valid\tAverage Loss:  0.008237748040872461\n",
      "\tEpoch 138  train\tAverage Loss:  0.006945193537643978\n",
      "\tEpoch 138  valid\tAverage Loss:  0.007017775493509629\n",
      "\tEpoch 139  train\tAverage Loss:  0.00637430648292814\n",
      "\tEpoch 139  valid\tAverage Loss:  0.006629967829760383\n",
      "\tEpoch 140  train\tAverage Loss:  0.0062708746109689985\n",
      "\tEpoch 140  valid\tAverage Loss:  0.006867643910295823\n",
      "\tEpoch 141  train\tAverage Loss:  0.006264906568186623\n",
      "\tEpoch 141  valid\tAverage Loss:  0.006654247694155749\n",
      "\tEpoch 142  train\tAverage Loss:  0.00622398772409984\n",
      "\tEpoch 142  valid\tAverage Loss:  0.006640549196916468\n",
      "\tEpoch 143  train\tAverage Loss:  0.006429031193256378\n",
      "\tEpoch 143  valid\tAverage Loss:  0.008030345229541554\n",
      "\tEpoch 144  train\tAverage Loss:  0.006959864377975464\n",
      "\tEpoch 144  valid\tAverage Loss:  0.00681362146840376\n",
      "\tEpoch 145  train\tAverage Loss:  0.006291278779506683\n",
      "\tEpoch 145  valid\tAverage Loss:  0.006663431086960961\n",
      "\tEpoch 146  train\tAverage Loss:  0.006218219867774419\n",
      "\tEpoch 146  valid\tAverage Loss:  0.006638100638109095\n",
      "\tEpoch 147  train\tAverage Loss:  0.006429274337632316\n",
      "\tEpoch 147  valid\tAverage Loss:  0.008175944195074194\n",
      "\tEpoch 148  train\tAverage Loss:  0.006932808356625693\n",
      "\tEpoch 148  valid\tAverage Loss:  0.007080693034564748\n",
      "\tEpoch 149  train\tAverage Loss:  0.006541195894990649\n",
      "\tEpoch 149  valid\tAverage Loss:  0.006664740057552562\n",
      "\tEpoch 150  train\tAverage Loss:  0.006333602888243539\n",
      "\tEpoch 150  valid\tAverage Loss:  0.007933441295343287\n",
      "Training complete!\n",
      "training network...\n",
      "\tEpoch 1  train\tAverage Loss:  0.006836476819855827\n",
      "\tEpoch 1  valid\tAverage Loss:  0.0071888577587464276\n",
      "\tEpoch 2  train\tAverage Loss:  0.006716403876032148\n",
      "\tEpoch 2  valid\tAverage Loss:  0.007724621436175178\n",
      "\tEpoch 3  train\tAverage Loss:  0.006773008099624089\n",
      "\tEpoch 3  valid\tAverage Loss:  0.007555299155852374\n",
      "\tEpoch 4  train\tAverage Loss:  0.006618462809494564\n",
      "\tEpoch 4  valid\tAverage Loss:  0.007191355929655187\n",
      "\tEpoch 5  train\tAverage Loss:  0.0066383929678371974\n",
      "\tEpoch 5  valid\tAverage Loss:  0.007896317629253163\n",
      "\tEpoch 6  train\tAverage Loss:  0.006562903566019876\n",
      "\tEpoch 6  valid\tAverage Loss:  0.007241662425153395\n",
      "\tEpoch 7  train\tAverage Loss:  0.006599700016634805\n",
      "\tEpoch 7  valid\tAverage Loss:  0.007914658329066107\n",
      "\tEpoch 8  train\tAverage Loss:  0.006406011700630188\n",
      "\tEpoch 8  valid\tAverage Loss:  0.006759962281760047\n",
      "\tEpoch 9  train\tAverage Loss:  0.006208641367299216\n",
      "\tEpoch 9  valid\tAverage Loss:  0.006654281300656936\n",
      "\tEpoch 10  train\tAverage Loss:  0.006215134961264474\n",
      "\tEpoch 10  valid\tAverage Loss:  0.007418803642777836\n",
      "\tEpoch 11  train\tAverage Loss:  0.006607744915144784\n",
      "\tEpoch 11  valid\tAverage Loss:  0.00796762161395129\n",
      "\tEpoch 12  train\tAverage Loss:  0.006387107959815434\n",
      "\tEpoch 12  valid\tAverage Loss:  0.006738109851584715\n",
      "\tEpoch 13  train\tAverage Loss:  0.006189662558691842\n",
      "\tEpoch 13  valid\tAverage Loss:  0.0066624935409602\n",
      "\tEpoch 14  train\tAverage Loss:  0.006220963248184749\n",
      "\tEpoch 14  valid\tAverage Loss:  0.007471607607953689\n",
      "\tEpoch 15  train\tAverage Loss:  0.006620219784123557\n",
      "\tEpoch 15  valid\tAverage Loss:  0.008068502825849196\n",
      "\tEpoch 16  train\tAverage Loss:  0.006337366546903337\n",
      "\tEpoch 16  valid\tAverage Loss:  0.006707948831950917\n",
      "\tEpoch 17  train\tAverage Loss:  0.006158537694386073\n",
      "\tEpoch 17  valid\tAverage Loss:  0.0066784957577200495\n",
      "\tEpoch 18  train\tAverage Loss:  0.006214089947087424\n",
      "\tEpoch 18  valid\tAverage Loss:  0.00741082531564376\n",
      "\tEpoch 19  train\tAverage Loss:  0.006624731148992266\n",
      "\tEpoch 19  valid\tAverage Loss:  0.008139603979447308\n",
      "\tEpoch 20  train\tAverage Loss:  0.0062965086783681595\n",
      "\tEpoch 20  valid\tAverage Loss:  0.00674289824331508\n",
      "\tEpoch 21  train\tAverage Loss:  0.00619351783820561\n",
      "\tEpoch 21  valid\tAverage Loss:  0.006692714165238773\n",
      "\tEpoch 22  train\tAverage Loss:  0.006187863545758384\n",
      "\tEpoch 22  valid\tAverage Loss:  0.007201307135469773\n",
      "\tEpoch 23  train\tAverage Loss:  0.0065935292073658535\n",
      "\tEpoch 23  valid\tAverage Loss:  0.008168849243837244\n",
      "\tEpoch 24  train\tAverage Loss:  0.006286250276224954\n",
      "\tEpoch 24  valid\tAverage Loss:  0.006780686834279228\n",
      "\tEpoch 25  train\tAverage Loss:  0.006261064393179757\n",
      "\tEpoch 25  valid\tAverage Loss:  0.006822712175986346\n",
      "\tEpoch 26  train\tAverage Loss:  0.006176413672310965\n",
      "\tEpoch 26  valid\tAverage Loss:  0.006714203620658201\n",
      "\tEpoch 27  train\tAverage Loss:  0.006159700768334525\n",
      "\tEpoch 27  valid\tAverage Loss:  0.006693701323340921\n",
      "\tEpoch 28  train\tAverage Loss:  0.006189146280288696\n",
      "\tEpoch 28  valid\tAverage Loss:  0.0072180380540735585\n",
      "\tEpoch 29  train\tAverage Loss:  0.0066061053020613535\n",
      "\tEpoch 29  valid\tAverage Loss:  0.008199277660425972\n",
      "\tEpoch 30  train\tAverage Loss:  0.006279314373220716\n",
      "\tEpoch 30  valid\tAverage Loss:  0.0067995507050963005\n",
      "\tEpoch 31  train\tAverage Loss:  0.00629666313954762\n",
      "\tEpoch 31  valid\tAverage Loss:  0.007044302751036251\n",
      "\tEpoch 32  train\tAverage Loss:  0.006363445179803031\n",
      "\tEpoch 32  valid\tAverage Loss:  0.007201040457276737\n",
      "\tEpoch 33  train\tAverage Loss:  0.0065432532003947665\n",
      "\tEpoch 33  valid\tAverage Loss:  0.008114786779179293\n",
      "\tEpoch 34  train\tAverage Loss:  0.006273531913757324\n",
      "\tEpoch 34  valid\tAverage Loss:  0.0074962178749196666\n",
      "\tEpoch 35  train\tAverage Loss:  0.00639373768227441\n",
      "\tEpoch 35  valid\tAverage Loss:  0.007808366452946382\n",
      "\tEpoch 36  train\tAverage Loss:  0.006240653055054801\n",
      "\tEpoch 36  valid\tAverage Loss:  0.007021575324675616\n",
      "\tEpoch 37  train\tAverage Loss:  0.006349784578595843\n",
      "\tEpoch 37  valid\tAverage Loss:  0.007821267457569347\n",
      "\tEpoch 38  train\tAverage Loss:  0.006363680669239589\n",
      "\tEpoch 38  valid\tAverage Loss:  0.007570178894435658\n",
      "\tEpoch 39  train\tAverage Loss:  0.006378415550504412\n",
      "\tEpoch 39  valid\tAverage Loss:  0.007797902401755838\n",
      "\tEpoch 40  train\tAverage Loss:  0.00622924461535045\n",
      "\tEpoch 40  valid\tAverage Loss:  0.007378573207294239\n",
      "\tEpoch 41  train\tAverage Loss:  0.006319435681615557\n",
      "\tEpoch 41  valid\tAverage Loss:  0.007736166645498837\n",
      "\tEpoch 42  train\tAverage Loss:  0.00621795643227441\n",
      "\tEpoch 42  valid\tAverage Loss:  0.007363352880758398\n",
      "\tEpoch 43  train\tAverage Loss:  0.006272322135312217\n",
      "\tEpoch 43  valid\tAverage Loss:  0.0075567914107266595\n",
      "\tEpoch 44  train\tAverage Loss:  0.006215009587151664\n",
      "\tEpoch 44  valid\tAverage Loss:  0.007281944295939277\n",
      "\tEpoch 45  train\tAverage Loss:  0.006234247497149876\n",
      "\tEpoch 45  valid\tAverage Loss:  0.007636736701516544\n",
      "\tEpoch 46  train\tAverage Loss:  0.0061984160627637595\n",
      "\tEpoch 46  valid\tAverage Loss:  0.006787229390705333\n",
      "\tEpoch 47  train\tAverage Loss:  0.006312229147979191\n",
      "\tEpoch 47  valid\tAverage Loss:  0.007846643258543576\n",
      "\tEpoch 48  train\tAverage Loss:  0.006233676203659603\n",
      "\tEpoch 48  valid\tAverage Loss:  0.006918048560619354\n",
      "\tEpoch 49  train\tAverage Loss:  0.006288687152521951\n",
      "\tEpoch 49  valid\tAverage Loss:  0.007537051123731276\n",
      "\tEpoch 50  train\tAverage Loss:  0.006338030082838876\n",
      "\tEpoch 50  valid\tAverage Loss:  0.007630388947094188\n",
      "\tEpoch 51  train\tAverage Loss:  0.006244917094707489\n",
      "\tEpoch 51  valid\tAverage Loss:  0.007308409003650441\n",
      "\tEpoch 52  train\tAverage Loss:  0.006276028505393437\n",
      "\tEpoch 52  valid\tAverage Loss:  0.007561006616143619\n",
      "\tEpoch 53  train\tAverage Loss:  0.006205945568425315\n",
      "\tEpoch 53  valid\tAverage Loss:  0.007331560289158541\n",
      "\tEpoch 54  train\tAverage Loss:  0.006239264224256788\n",
      "\tEpoch 54  valid\tAverage Loss:  0.007510684974053327\n",
      "\tEpoch 55  train\tAverage Loss:  0.006190995522907802\n",
      "\tEpoch 55  valid\tAverage Loss:  0.007398539255647098\n",
      "\tEpoch 56  train\tAverage Loss:  0.006219645857810974\n",
      "\tEpoch 56  valid\tAverage Loss:  0.007479236441500047\n",
      "\tEpoch 57  train\tAverage Loss:  0.00618645349570683\n",
      "\tEpoch 57  valid\tAverage Loss:  0.007452163310611949\n",
      "\tEpoch 58  train\tAverage Loss:  0.006198485323360988\n",
      "\tEpoch 58  valid\tAverage Loss:  0.007428732864996966\n",
      "\tEpoch 59  train\tAverage Loss:  0.006177450094904218\n",
      "\tEpoch 59  valid\tAverage Loss:  0.007454096254180459\n",
      "\tEpoch 60  train\tAverage Loss:  0.006186069939817701\n",
      "\tEpoch 60  valid\tAverage Loss:  0.007446102815515855\n",
      "\tEpoch 61  train\tAverage Loss:  0.006173630382333483\n",
      "\tEpoch 61  valid\tAverage Loss:  0.007479027264258441\n",
      "\tEpoch 62  train\tAverage Loss:  0.006177451031548637\n",
      "\tEpoch 62  valid\tAverage Loss:  0.007480500831323511\n",
      "\tEpoch 63  train\tAverage Loss:  0.006172646454402379\n",
      "\tEpoch 63  valid\tAverage Loss:  0.00751776867053088\n",
      "\tEpoch 64  train\tAverage Loss:  0.0061740853445870535\n",
      "\tEpoch 64  valid\tAverage Loss:  0.007499167182866265\n",
      "\tEpoch 65  train\tAverage Loss:  0.006178363723414285\n",
      "\tEpoch 65  valid\tAverage Loss:  0.007549678437850055\n",
      "\tEpoch 66  train\tAverage Loss:  0.006174228038106646\n",
      "\tEpoch 66  valid\tAverage Loss:  0.00753686950487249\n",
      "\tEpoch 67  train\tAverage Loss:  0.006182669111660549\n",
      "\tEpoch 67  valid\tAverage Loss:  0.007542931788107928\n",
      "\tEpoch 68  train\tAverage Loss:  0.006172816898141588\n",
      "\tEpoch 68  valid\tAverage Loss:  0.00755777755204369\n",
      "\tEpoch 69  train\tAverage Loss:  0.006179538854530879\n",
      "\tEpoch 69  valid\tAverage Loss:  0.00752286483259762\n",
      "\tEpoch 70  train\tAverage Loss:  0.006174013997827257\n",
      "\tEpoch 70  valid\tAverage Loss:  0.007577178267871632\n",
      "\tEpoch 71  train\tAverage Loss:  0.006183719388076238\n",
      "\tEpoch 71  valid\tAverage Loss:  0.007486662058269276\n",
      "\tEpoch 72  train\tAverage Loss:  0.006169857348714556\n",
      "\tEpoch 72  valid\tAverage Loss:  0.007575258998309865\n",
      "\tEpoch 73  train\tAverage Loss:  0.006180459780352456\n",
      "\tEpoch 73  valid\tAverage Loss:  0.0074533217444139365\n",
      "\tEpoch 74  train\tAverage Loss:  0.006163422984736306\n",
      "\tEpoch 74  valid\tAverage Loss:  0.007547644481939428\n",
      "\tEpoch 75  train\tAverage Loss:  0.006173953073365348\n",
      "\tEpoch 75  valid\tAverage Loss:  0.007433991923051722\n",
      "\tEpoch 76  train\tAverage Loss:  0.006178920183862959\n",
      "\tEpoch 76  valid\tAverage Loss:  0.007569543263491462\n",
      "\tEpoch 77  train\tAverage Loss:  0.006167545412267958\n",
      "\tEpoch 77  valid\tAverage Loss:  0.007413106806137983\n",
      "\tEpoch 78  train\tAverage Loss:  0.006159157804080418\n",
      "\tEpoch 78  valid\tAverage Loss:  0.007489678684402915\n",
      "\tEpoch 79  train\tAverage Loss:  0.006161332717963627\n",
      "\tEpoch 79  valid\tAverage Loss:  0.00736949391224805\n",
      "\tEpoch 80  train\tAverage Loss:  0.006151578979832786\n",
      "\tEpoch 80  valid\tAverage Loss:  0.00748133568202748\n",
      "\tEpoch 81  train\tAverage Loss:  0.006156987011432647\n",
      "\tEpoch 81  valid\tAverage Loss:  0.007350296728751238\n",
      "\tEpoch 82  train\tAverage Loss:  0.006149274400302342\n",
      "\tEpoch 82  valid\tAverage Loss:  0.007485254371867461\n",
      "\tEpoch 83  train\tAverage Loss:  0.006152630269527435\n",
      "\tEpoch 83  valid\tAverage Loss:  0.007320332807653091\n",
      "\tEpoch 84  train\tAverage Loss:  0.006131532379559108\n",
      "\tEpoch 84  valid\tAverage Loss:  0.007458425094099606\n",
      "\tEpoch 85  train\tAverage Loss:  0.006144857321466719\n",
      "\tEpoch 85  valid\tAverage Loss:  0.007321381849401137\n",
      "\tEpoch 86  train\tAverage Loss:  0.0061273360507828846\n",
      "\tEpoch 86  valid\tAverage Loss:  0.007418919977019815\n",
      "\tEpoch 87  train\tAverage Loss:  0.006135698139667511\n",
      "\tEpoch 87  valid\tAverage Loss:  0.007318126138518838\n",
      "\tEpoch 88  train\tAverage Loss:  0.006122345856257847\n",
      "\tEpoch 88  valid\tAverage Loss:  0.007395012869554407\n",
      "\tEpoch 89  train\tAverage Loss:  0.006128033442156656\n",
      "\tEpoch 89  valid\tAverage Loss:  0.007312400831895716\n",
      "\tEpoch 90  train\tAverage Loss:  0.006118395830903734\n",
      "\tEpoch 90  valid\tAverage Loss:  0.007363676043117748\n",
      "\tEpoch 91  train\tAverage Loss:  0.006116643999304091\n",
      "\tEpoch 91  valid\tAverage Loss:  0.007285111686762641\n",
      "\tEpoch 92  train\tAverage Loss:  0.006111533241612571\n",
      "\tEpoch 92  valid\tAverage Loss:  0.007347140207010157\n",
      "\tEpoch 93  train\tAverage Loss:  0.006112485306603568\n",
      "\tEpoch 93  valid\tAverage Loss:  0.007262981989804436\n",
      "\tEpoch 94  train\tAverage Loss:  0.006168265453406743\n",
      "\tEpoch 94  valid\tAverage Loss:  0.007025690604658688\n",
      "\tEpoch 95  train\tAverage Loss:  0.006090928035123008\n",
      "\tEpoch 95  valid\tAverage Loss:  0.007447357423165265\n",
      "\tEpoch 96  train\tAverage Loss:  0.00612225192785263\n",
      "\tEpoch 96  valid\tAverage Loss:  0.007278697806246141\n",
      "\tEpoch 97  train\tAverage Loss:  0.006101667489324297\n",
      "\tEpoch 97  valid\tAverage Loss:  0.007248045276193058\n",
      "\tEpoch 98  train\tAverage Loss:  0.006103607518332345\n",
      "\tEpoch 98  valid\tAverage Loss:  0.007312179838909822\n",
      "\tEpoch 99  train\tAverage Loss:  0.006102192529610225\n",
      "\tEpoch 99  valid\tAverage Loss:  0.00723717905142728\n",
      "\tEpoch 100  train\tAverage Loss:  0.006096151777676174\n",
      "\tEpoch 100  valid\tAverage Loss:  0.007305376161547268\n",
      "\tEpoch 101  train\tAverage Loss:  0.006099161914416722\n",
      "\tEpoch 101  valid\tAverage Loss:  0.00722661893157398\n",
      "\tEpoch 102  train\tAverage Loss:  0.0060932352798325675\n",
      "\tEpoch 102  valid\tAverage Loss:  0.007315665378290064\n",
      "\tEpoch 103  train\tAverage Loss:  0.006097151219844818\n",
      "\tEpoch 103  valid\tAverage Loss:  0.007222824377172134\n",
      "\tEpoch 104  train\tAverage Loss:  0.006088413281100137\n",
      "\tEpoch 104  valid\tAverage Loss:  0.007289908160181606\n",
      "\tEpoch 105  train\tAverage Loss:  0.006094587802886963\n",
      "\tEpoch 105  valid\tAverage Loss:  0.007214024101986604\n",
      "\tEpoch 106  train\tAverage Loss:  0.006079855527196611\n",
      "\tEpoch 106  valid\tAverage Loss:  0.007286186814308166\n",
      "\tEpoch 107  train\tAverage Loss:  0.006087526432105473\n",
      "\tEpoch 107  valid\tAverage Loss:  0.007225014742682962\n",
      "\tEpoch 108  train\tAverage Loss:  0.0060773984449250355\n",
      "\tEpoch 108  valid\tAverage Loss:  0.0072540965150384345\n",
      "\tEpoch 109  train\tAverage Loss:  0.006082908647400992\n",
      "\tEpoch 109  valid\tAverage Loss:  0.007219399616998785\n",
      "\tEpoch 110  train\tAverage Loss:  0.006074742947305952\n",
      "\tEpoch 110  valid\tAverage Loss:  0.007243167705395642\n",
      "\tEpoch 111  train\tAverage Loss:  0.0060806751251220705\n",
      "\tEpoch 111  valid\tAverage Loss:  0.007220044767155367\n",
      "\tEpoch 112  train\tAverage Loss:  0.006069780485970633\n",
      "\tEpoch 112  valid\tAverage Loss:  0.007248273158774656\n",
      "\tEpoch 113  train\tAverage Loss:  0.006080778138978141\n",
      "\tEpoch 113  valid\tAverage Loss:  0.007213395237922669\n",
      "\tEpoch 114  train\tAverage Loss:  0.006067780230726514\n",
      "\tEpoch 114  valid\tAverage Loss:  0.007236059150275062\n",
      "\tEpoch 115  train\tAverage Loss:  0.00607876900264195\n",
      "\tEpoch 115  valid\tAverage Loss:  0.007216677367687226\n",
      "\tEpoch 116  train\tAverage Loss:  0.006067516948495592\n",
      "\tEpoch 116  valid\tAverage Loss:  0.0072357094813795654\n",
      "\tEpoch 117  train\tAverage Loss:  0.006077900980200087\n",
      "\tEpoch 117  valid\tAverage Loss:  0.00722237897269866\n",
      "\tEpoch 118  train\tAverage Loss:  0.006070833086967468\n",
      "\tEpoch 118  valid\tAverage Loss:  0.007216800240909352\n",
      "\tEpoch 119  train\tAverage Loss:  0.0060719489284924094\n",
      "\tEpoch 119  valid\tAverage Loss:  0.00722360107828589\n",
      "\tEpoch 120  train\tAverage Loss:  0.006073007064206259\n",
      "\tEpoch 120  valid\tAverage Loss:  0.007230438677703633\n",
      "\tEpoch 121  train\tAverage Loss:  0.006069525011948177\n",
      "\tEpoch 121  valid\tAverage Loss:  0.007223247570150039\n",
      "\tEpoch 122  train\tAverage Loss:  0.006078636177948543\n",
      "\tEpoch 122  valid\tAverage Loss:  0.007225837409496307\n",
      "\tEpoch 123  train\tAverage Loss:  0.006064173672880445\n",
      "\tEpoch 123  valid\tAverage Loss:  0.007219106204369489\n",
      "\tEpoch 124  train\tAverage Loss:  0.006096914308411735\n",
      "\tEpoch 124  valid\tAverage Loss:  0.007268195432775161\n",
      "\tEpoch 125  train\tAverage Loss:  0.006083413864885058\n",
      "\tEpoch 125  valid\tAverage Loss:  0.007228649206021253\n",
      "\tEpoch 126  train\tAverage Loss:  0.006121056250163487\n",
      "\tEpoch 126  valid\tAverage Loss:  0.0073001868584576775\n",
      "\tEpoch 127  train\tAverage Loss:  0.006145274196352277\n",
      "\tEpoch 127  valid\tAverage Loss:  0.007269161431228413\n",
      "\tEpoch 128  train\tAverage Loss:  0.0063226627622331895\n",
      "\tEpoch 128  valid\tAverage Loss:  0.007169074468752917\n",
      "\tEpoch 129  train\tAverage Loss:  0.006499118745326996\n",
      "\tEpoch 129  valid\tAverage Loss:  0.0068530892273958996\n",
      "\tEpoch 130  train\tAverage Loss:  0.006505453203405653\n",
      "\tEpoch 130  valid\tAverage Loss:  0.007094716236871831\n",
      "\tEpoch 131  train\tAverage Loss:  0.006210529429571969\n",
      "\tEpoch 131  valid\tAverage Loss:  0.006728034264901105\n",
      "\tEpoch 132  train\tAverage Loss:  0.006530499347618648\n",
      "\tEpoch 132  valid\tAverage Loss:  0.0068513641988529874\n",
      "\tEpoch 133  train\tAverage Loss:  0.0067742816039494105\n",
      "\tEpoch 133  valid\tAverage Loss:  0.0071170965713613175\n",
      "\tEpoch 134  train\tAverage Loss:  0.006555617553847177\n",
      "\tEpoch 134  valid\tAverage Loss:  0.006908478982308332\n",
      "\tEpoch 135  train\tAverage Loss:  0.006353718434061323\n",
      "\tEpoch 135  valid\tAverage Loss:  0.00686562296222238\n",
      "\tEpoch 136  train\tAverage Loss:  0.006255520718438285\n",
      "\tEpoch 136  valid\tAverage Loss:  0.006826033346793231\n",
      "\tEpoch 137  train\tAverage Loss:  0.006206120703901564\n",
      "\tEpoch 137  valid\tAverage Loss:  0.006805021622601677\n",
      "\tEpoch 138  train\tAverage Loss:  0.006145645661013467\n",
      "\tEpoch 138  valid\tAverage Loss:  0.0067831390394884\n",
      "\tEpoch 139  train\tAverage Loss:  0.006128912695816585\n",
      "\tEpoch 139  valid\tAverage Loss:  0.006767016473938437\n",
      "\tEpoch 140  train\tAverage Loss:  0.006102344853537423\n",
      "\tEpoch 140  valid\tAverage Loss:  0.006753800195806166\n",
      "\tEpoch 141  train\tAverage Loss:  0.006094095621790205\n",
      "\tEpoch 141  valid\tAverage Loss:  0.006760211797321544\n",
      "\tEpoch 142  train\tAverage Loss:  0.006075166063649314\n",
      "\tEpoch 142  valid\tAverage Loss:  0.006743209362030029\n",
      "\tEpoch 143  train\tAverage Loss:  0.006070008899484362\n",
      "\tEpoch 143  valid\tAverage Loss:  0.0067532958353267\n",
      "\tEpoch 144  train\tAverage Loss:  0.006083215730530875\n",
      "\tEpoch 144  valid\tAverage Loss:  0.006733358502388\n",
      "\tEpoch 145  train\tAverage Loss:  0.006048317474978311\n",
      "\tEpoch 145  valid\tAverage Loss:  0.0067459333293578205\n",
      "\tEpoch 146  train\tAverage Loss:  0.006054180553981236\n",
      "\tEpoch 146  valid\tAverage Loss:  0.006736238458577324\n",
      "\tEpoch 147  train\tAverage Loss:  0.006041081496647426\n",
      "\tEpoch 147  valid\tAverage Loss:  0.006737103882957908\n",
      "\tEpoch 148  train\tAverage Loss:  0.006050678755555834\n",
      "\tEpoch 148  valid\tAverage Loss:  0.006735992081025067\n",
      "\tEpoch 149  train\tAverage Loss:  0.006029158643313817\n",
      "\tEpoch 149  valid\tAverage Loss:  0.0067347185401355515\n",
      "\tEpoch 150  train\tAverage Loss:  0.0060408463222639906\n",
      "\tEpoch 150  valid\tAverage Loss:  0.00673468877287472\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "network.train()\n",
    "\n",
    "print(\"training network...\")\n",
    "for epoch in range(epochs):\n",
    "    train_overall_loss = 0\n",
    "\n",
    "    valid_overall_loss = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = network(x)\n",
    "        loss = loss_function(out,y.reshape(-1,1))\n",
    "\n",
    "        train_overall_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "      for valid_batch_idx, (valid_x, valid_y) in enumerate(valid_loader):\n",
    "        valid_x = valid_x.to(device)\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "\n",
    "        valid_out = network(valid_x)\n",
    "        valid_loss = loss_function(valid_out, valid_y.reshape(-1,1))\n",
    "\n",
    "        valid_overall_loss += valid_loss.item()\n",
    "\n",
    "    train_n_datapoints = batch_idx * batch_size\n",
    "    valid_n_datapoints = valid_batch_idx * batch_size\n",
    "\n",
    "    print(\"\\tEpoch\", epoch + 1, \" train\\tAverage Loss: \", train_overall_loss / train_n_datapoints)\n",
    "    print(\"\\tEpoch\", epoch + 1, \" valid\\tAverage Loss: \", valid_overall_loss / valid_n_datapoints)\n",
    "\n",
    "print(\"Training complete!\")\n",
    "# to save the model, uncomment the line below.\n",
    "# torch.save(network,\"./network.pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing on the test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:00<00:00, 20.10it/s]\n"
     ]
    }
   ],
   "source": [
    "network.eval()\n",
    "\n",
    "x_original_list = []\n",
    "y_list = []\n",
    "out_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (test_x, test_y) in enumerate(tqdm(test_loader)):\n",
    "        test_x = test_x.to(device)\n",
    "\n",
    "        test_out = network(test_x)\n",
    "        x_original_list.append(test_x)\n",
    "        y_list.append(test_y)\n",
    "        out_list.append(test_out)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Converting output and the target (y) into 1D tensors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1767])\n",
      "torch.Size([1767])\n"
     ]
    }
   ],
   "source": [
    "y_tensor = torch.Tensor()\n",
    "out_tensor = torch.Tensor()\n",
    "\n",
    "for i in range(len(out_list)):\n",
    "    out_tensor = torch.cat((out_tensor,out_list[i]))\n",
    "    y_tensor = torch.cat((y_tensor,y_list[i]))\n",
    "\n",
    "out_tensor = torch.where(out_tensor<0.5,0,1).reshape((-1,))\n",
    "y_tensor = y_tensor.reshape((-1,))\n",
    "print(out_tensor.shape)\n",
    "print(y_tensor.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "F1 score calculation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0050)\n"
     ]
    }
   ],
   "source": [
    "f1 = torchmetrics.F1Score(task=\"binary\",num_classes=2)\n",
    "score = f1(out_tensor,y_tensor)\n",
    "print(score)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Confusion Matrix"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1371,   20],\n",
      "        [ 375,    1]])\n"
     ]
    }
   ],
   "source": [
    "confusion = torchmetrics.ConfusionMatrix(task=\"binary\",num_classes=2)\n",
    "conf_matrix = confusion(out_tensor,y_tensor)\n",
    "print(conf_matrix)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7765)\n"
     ]
    }
   ],
   "source": [
    "acc = torchmetrics.Accuracy(task=\"binary\",num_classes=2)\n",
    "accuracy = acc(out_tensor,y_tensor)\n",
    "print(accuracy)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
