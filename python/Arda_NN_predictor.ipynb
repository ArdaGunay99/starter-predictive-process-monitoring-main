{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"ML_data2.csv\",header=None)\n",
    "data\n",
    "data = data.to_numpy()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_valtest, y_train, y_valtest = train_test_split(data[:,:-1], data[:,-1],\n",
    "                                                                                  train_size=0.5, shuffle=False)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_valtest, y_valtest, train_size=0.5, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "y_train = np.where(y_train<=0,0,1)\n",
    "y_val = np.where(y_val<=0,0,1)\n",
    "y_test = np.where(y_test<=0,0,1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "X_train_tensor = TensorDataset(torch.Tensor(X_train),torch.Tensor(y_train))\n",
    "X_val_tensor = TensorDataset(torch.Tensor(X_val),torch.Tensor(y_val))\n",
    "X_test_tensor = TensorDataset(torch.Tensor(X_test),torch.Tensor(y_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# creating dataloaders for the datasets\n",
    "\n",
    "# arguments for the dataloaders\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "batch_size = 100\n",
    "# dataloaders\n",
    "train_loader = DataLoader(dataset=X_train_tensor, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "valid_loader = DataLoader(dataset=X_val_tensor, batch_size=batch_size, shuffle=False, **kwargs)\n",
    "test_loader  = DataLoader(dataset=X_test_tensor ,  batch_size=batch_size, shuffle=False,  **kwargs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 4]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "batch_x, batch_y = next(iter(train_loader))\n",
    "print(batch_x.shape, batch_y.shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.linear2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.linear3 = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = torch.sigmoid(self.linear3(x))\n",
    "        return x\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "hidden_dim_1 = 64\n",
    "hidden_dim_2 = 64\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "network = Network(4,hidden_dim_1,hidden_dim_2,1)\n",
    "\n",
    "loss_function = F.binary_cross_entropy\n",
    "optimizer = Adam(network.parameters(), lr=lr)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training network...\n",
      "\tEpoch 1  train\tAverage Loss:  0.13923038046700614\n",
      "\tEpoch 1  valid\tAverage Loss:  0.1170101676267736\n",
      "\tEpoch 2  train\tAverage Loss:  0.07589639057431902\n",
      "\tEpoch 2  valid\tAverage Loss:  0.029619554351357852\n",
      "\tEpoch 3  train\tAverage Loss:  0.04317743395056043\n",
      "\tEpoch 3  valid\tAverage Loss:  0.03086438312250025\n",
      "\tEpoch 4  train\tAverage Loss:  0.033954346333231245\n",
      "\tEpoch 4  valid\tAverage Loss:  0.012188010426128612\n",
      "\tEpoch 5  train\tAverage Loss:  0.018873175723212107\n",
      "\tEpoch 5  valid\tAverage Loss:  0.033687948198879464\n",
      "\tEpoch 6  train\tAverage Loss:  0.024774311661720276\n",
      "\tEpoch 6  valid\tAverage Loss:  0.007106264408896951\n",
      "\tEpoch 7  train\tAverage Loss:  0.010083911010197231\n",
      "\tEpoch 7  valid\tAverage Loss:  0.009170269159709706\n",
      "\tEpoch 8  train\tAverage Loss:  0.011524397909641266\n",
      "\tEpoch 8  valid\tAverage Loss:  0.03492107755997602\n",
      "\tEpoch 9  train\tAverage Loss:  0.03539837086200714\n",
      "\tEpoch 9  valid\tAverage Loss:  0.034444313470055075\n",
      "\tEpoch 10  train\tAverage Loss:  0.015445027726037161\n",
      "\tEpoch 10  valid\tAverage Loss:  0.012151758530560662\n",
      "\tEpoch 11  train\tAverage Loss:  0.012261013490813119\n",
      "\tEpoch 11  valid\tAverage Loss:  0.008141340438057395\n",
      "\tEpoch 12  train\tAverage Loss:  0.00922622252362115\n",
      "\tEpoch 12  valid\tAverage Loss:  0.009442153958713307\n",
      "\tEpoch 13  train\tAverage Loss:  0.013488312729767391\n",
      "\tEpoch 13  valid\tAverage Loss:  0.031145330597372615\n",
      "\tEpoch 14  train\tAverage Loss:  0.023407816103526526\n",
      "\tEpoch 14  valid\tAverage Loss:  0.06035473444882561\n",
      "\tEpoch 15  train\tAverage Loss:  0.018407253657068526\n",
      "\tEpoch 15  valid\tAverage Loss:  0.021695003158905928\n",
      "\tEpoch 16  train\tAverage Loss:  0.011223784412656512\n",
      "\tEpoch 16  valid\tAverage Loss:  0.007760327283073874\n",
      "\tEpoch 17  train\tAverage Loss:  0.007850042462348938\n",
      "\tEpoch 17  valid\tAverage Loss:  0.01126614346223719\n",
      "\tEpoch 18  train\tAverage Loss:  0.010700661488941738\n",
      "\tEpoch 18  valid\tAverage Loss:  0.008194506939719706\n",
      "\tEpoch 19  train\tAverage Loss:  0.009015197072710309\n",
      "\tEpoch 19  valid\tAverage Loss:  0.01016081690788269\n",
      "\tEpoch 20  train\tAverage Loss:  0.01206267296416419\n",
      "\tEpoch 20  valid\tAverage Loss:  0.028480103436638326\n",
      "\tEpoch 21  train\tAverage Loss:  0.019991056902068\n",
      "\tEpoch 21  valid\tAverage Loss:  0.007431909617255716\n",
      "\tEpoch 22  train\tAverage Loss:  0.012362281816346304\n",
      "\tEpoch 22  valid\tAverage Loss:  0.02155812796424417\n",
      "\tEpoch 23  train\tAverage Loss:  0.016197165284837996\n",
      "\tEpoch 23  valid\tAverage Loss:  0.020460766624001897\n",
      "\tEpoch 24  train\tAverage Loss:  0.01812421793597085\n",
      "\tEpoch 24  valid\tAverage Loss:  0.007253880746224347\n",
      "\tEpoch 25  train\tAverage Loss:  0.013314551625932966\n",
      "\tEpoch 25  valid\tAverage Loss:  0.006737512104651507\n",
      "\tEpoch 26  train\tAverage Loss:  0.008357202001980374\n",
      "\tEpoch 26  valid\tAverage Loss:  0.007680729557486141\n",
      "\tEpoch 27  train\tAverage Loss:  0.01011621962274824\n",
      "\tEpoch 27  valid\tAverage Loss:  0.011946555481237523\n",
      "\tEpoch 28  train\tAverage Loss:  0.008288121461868286\n",
      "\tEpoch 28  valid\tAverage Loss:  0.009689578238655539\n",
      "\tEpoch 29  train\tAverage Loss:  0.012745866000652313\n",
      "\tEpoch 29  valid\tAverage Loss:  0.012295958855572869\n",
      "\tEpoch 30  train\tAverage Loss:  0.011013401474271501\n",
      "\tEpoch 30  valid\tAverage Loss:  0.009988481998443604\n",
      "\tEpoch 31  train\tAverage Loss:  0.008182375856808253\n",
      "\tEpoch 31  valid\tAverage Loss:  0.00733927120180691\n",
      "\tEpoch 32  train\tAverage Loss:  0.009980479334081922\n",
      "\tEpoch 32  valid\tAverage Loss:  0.01029197237070869\n",
      "\tEpoch 33  train\tAverage Loss:  0.012481853408472878\n",
      "\tEpoch 33  valid\tAverage Loss:  0.012579914261313045\n",
      "\tEpoch 34  train\tAverage Loss:  0.021582607081958225\n",
      "\tEpoch 34  valid\tAverage Loss:  0.014372043188880471\n",
      "\tEpoch 35  train\tAverage Loss:  0.011243688259805951\n",
      "\tEpoch 35  valid\tAverage Loss:  0.007785852411214043\n",
      "\tEpoch 36  train\tAverage Loss:  0.007542257538863591\n",
      "\tEpoch 36  valid\tAverage Loss:  0.006849521433605867\n",
      "\tEpoch 37  train\tAverage Loss:  0.01190329041651317\n",
      "\tEpoch 37  valid\tAverage Loss:  0.014834220339270199\n",
      "\tEpoch 38  train\tAverage Loss:  0.015164865170206342\n",
      "\tEpoch 38  valid\tAverage Loss:  0.012137959704679601\n",
      "\tEpoch 39  train\tAverage Loss:  0.00838416530404772\n",
      "\tEpoch 39  valid\tAverage Loss:  0.006906655641163097\n",
      "\tEpoch 40  train\tAverage Loss:  0.007438962468079158\n",
      "\tEpoch 40  valid\tAverage Loss:  0.00856921374797821\n",
      "\tEpoch 41  train\tAverage Loss:  0.01247364925486701\n",
      "\tEpoch 41  valid\tAverage Loss:  0.025964716672897337\n",
      "\tEpoch 42  train\tAverage Loss:  0.011472292150769914\n",
      "\tEpoch 42  valid\tAverage Loss:  0.01172889414955588\n",
      "\tEpoch 43  train\tAverage Loss:  0.01110754975250789\n",
      "\tEpoch 43  valid\tAverage Loss:  0.021603218597524305\n",
      "\tEpoch 44  train\tAverage Loss:  0.010844786933490207\n",
      "\tEpoch 44  valid\tAverage Loss:  0.006636952863020055\n",
      "\tEpoch 45  train\tAverage Loss:  0.008422919963087354\n",
      "\tEpoch 45  valid\tAverage Loss:  0.01175173233537113\n",
      "\tEpoch 46  train\tAverage Loss:  0.008456498197146825\n",
      "\tEpoch 46  valid\tAverage Loss:  0.00809658103129443\n",
      "\tEpoch 47  train\tAverage Loss:  0.007495724201202393\n",
      "\tEpoch 47  valid\tAverage Loss:  0.008631071483387667\n",
      "\tEpoch 48  train\tAverage Loss:  0.007950288091387068\n",
      "\tEpoch 48  valid\tAverage Loss:  0.009002991248579586\n",
      "\tEpoch 49  train\tAverage Loss:  0.00785280795608248\n",
      "\tEpoch 49  valid\tAverage Loss:  0.007961685867870555\n",
      "\tEpoch 50  train\tAverage Loss:  0.00940424461875643\n",
      "\tEpoch 50  valid\tAverage Loss:  0.011667279320604662\n",
      "\tEpoch 51  train\tAverage Loss:  0.007809358767100743\n",
      "\tEpoch 51  valid\tAverage Loss:  0.008946982762392829\n",
      "\tEpoch 52  train\tAverage Loss:  0.008001802112374987\n",
      "\tEpoch 52  valid\tAverage Loss:  0.007056631796500262\n",
      "\tEpoch 53  train\tAverage Loss:  0.007284305930137634\n",
      "\tEpoch 53  valid\tAverage Loss:  0.007453992892714108\n",
      "\tEpoch 54  train\tAverage Loss:  0.010531703650951385\n",
      "\tEpoch 54  valid\tAverage Loss:  0.011981170948813943\n",
      "\tEpoch 55  train\tAverage Loss:  0.013207434500966753\n",
      "\tEpoch 55  valid\tAverage Loss:  0.018649919103173648\n",
      "\tEpoch 56  train\tAverage Loss:  0.007897427584443773\n",
      "\tEpoch 56  valid\tAverage Loss:  0.007692802758777843\n",
      "\tEpoch 57  train\tAverage Loss:  0.009998113785471234\n",
      "\tEpoch 57  valid\tAverage Loss:  0.014215227260309108\n",
      "\tEpoch 58  train\tAverage Loss:  0.015685778651918683\n",
      "\tEpoch 58  valid\tAverage Loss:  0.006675085320192225\n",
      "\tEpoch 59  train\tAverage Loss:  0.0093168334875788\n",
      "\tEpoch 59  valid\tAverage Loss:  0.009705102058017955\n",
      "\tEpoch 60  train\tAverage Loss:  0.008674551793507167\n",
      "\tEpoch 60  valid\tAverage Loss:  0.008658589475295122\n",
      "\tEpoch 61  train\tAverage Loss:  0.0076206851771899635\n",
      "\tEpoch 61  valid\tAverage Loss:  0.008233321098720325\n",
      "\tEpoch 62  train\tAverage Loss:  0.008268532497542246\n",
      "\tEpoch 62  valid\tAverage Loss:  0.012442147346103893\n",
      "\tEpoch 63  train\tAverage Loss:  0.00788046339579991\n",
      "\tEpoch 63  valid\tAverage Loss:  0.006915556963752297\n",
      "\tEpoch 64  train\tAverage Loss:  0.006808049934250967\n",
      "\tEpoch 64  valid\tAverage Loss:  0.006835687616292168\n",
      "\tEpoch 65  train\tAverage Loss:  0.009424972133977073\n",
      "\tEpoch 65  valid\tAverage Loss:  0.008761031347162584\n",
      "\tEpoch 66  train\tAverage Loss:  0.010657170738492693\n",
      "\tEpoch 66  valid\tAverage Loss:  0.012992030592525707\n",
      "\tEpoch 67  train\tAverage Loss:  0.01497804478236607\n",
      "\tEpoch 67  valid\tAverage Loss:  0.00959555587347816\n",
      "\tEpoch 68  train\tAverage Loss:  0.009615002146789006\n",
      "\tEpoch 68  valid\tAverage Loss:  0.011209239784409018\n",
      "\tEpoch 69  train\tAverage Loss:  0.007975688372339522\n",
      "\tEpoch 69  valid\tAverage Loss:  0.006788261918460621\n",
      "\tEpoch 70  train\tAverage Loss:  0.008100288169724601\n",
      "\tEpoch 70  valid\tAverage Loss:  0.006600484812960905\n",
      "\tEpoch 71  train\tAverage Loss:  0.007799630684512002\n",
      "\tEpoch 71  valid\tAverage Loss:  0.006705458935569315\n",
      "\tEpoch 72  train\tAverage Loss:  0.00839972700391497\n",
      "\tEpoch 72  valid\tAverage Loss:  0.010298334149753346\n",
      "\tEpoch 73  train\tAverage Loss:  0.009059964512075696\n",
      "\tEpoch 73  valid\tAverage Loss:  0.006699088215827942\n",
      "\tEpoch 74  train\tAverage Loss:  0.008494212363447462\n",
      "\tEpoch 74  valid\tAverage Loss:  0.011637943948016448\n",
      "\tEpoch 75  train\tAverage Loss:  0.00891966416154589\n",
      "\tEpoch 75  valid\tAverage Loss:  0.007440313521553488\n",
      "\tEpoch 76  train\tAverage Loss:  0.0077351933036531725\n",
      "\tEpoch 76  valid\tAverage Loss:  0.006696581524961135\n",
      "\tEpoch 77  train\tAverage Loss:  0.007167886682919094\n",
      "\tEpoch 77  valid\tAverage Loss:  0.006876661952804117\n",
      "\tEpoch 78  train\tAverage Loss:  0.008310912932668413\n",
      "\tEpoch 78  valid\tAverage Loss:  0.010116107744329117\n",
      "\tEpoch 79  train\tAverage Loss:  0.008854271599224636\n",
      "\tEpoch 79  valid\tAverage Loss:  0.006905852275736191\n",
      "\tEpoch 80  train\tAverage Loss:  0.00811503620658602\n",
      "\tEpoch 80  valid\tAverage Loss:  0.018346398437724395\n",
      "\tEpoch 81  train\tAverage Loss:  0.010370226161820548\n",
      "\tEpoch 81  valid\tAverage Loss:  0.0068426592209759884\n",
      "\tEpoch 82  train\tAverage Loss:  0.008072491560663496\n",
      "\tEpoch 82  valid\tAverage Loss:  0.008620684567619772\n",
      "\tEpoch 83  train\tAverage Loss:  0.00808829653263092\n",
      "\tEpoch 83  valid\tAverage Loss:  0.00864231074557585\n",
      "\tEpoch 84  train\tAverage Loss:  0.007864774789128984\n",
      "\tEpoch 84  valid\tAverage Loss:  0.007440362993408652\n",
      "\tEpoch 85  train\tAverage Loss:  0.008751046691622053\n",
      "\tEpoch 85  valid\tAverage Loss:  0.013230332276400398\n",
      "\tEpoch 86  train\tAverage Loss:  0.021747888837541852\n",
      "\tEpoch 86  valid\tAverage Loss:  0.010634589615990134\n",
      "\tEpoch 87  train\tAverage Loss:  0.007431334257125855\n",
      "\tEpoch 87  valid\tAverage Loss:  0.007544233834042268\n",
      "\tEpoch 88  train\tAverage Loss:  0.007196091226169041\n",
      "\tEpoch 88  valid\tAverage Loss:  0.006774202760528115\n",
      "\tEpoch 89  train\tAverage Loss:  0.007438560588019235\n",
      "\tEpoch 89  valid\tAverage Loss:  0.010696759083691765\n",
      "\tEpoch 90  train\tAverage Loss:  0.007628068694046566\n",
      "\tEpoch 90  valid\tAverage Loss:  0.00700424627346151\n",
      "\tEpoch 91  train\tAverage Loss:  0.007229502865246364\n",
      "\tEpoch 91  valid\tAverage Loss:  0.007239518831757938\n",
      "\tEpoch 92  train\tAverage Loss:  0.008361111513205937\n",
      "\tEpoch 92  valid\tAverage Loss:  0.011910279077642104\n",
      "\tEpoch 93  train\tAverage Loss:  0.008569677863802229\n",
      "\tEpoch 93  valid\tAverage Loss:  0.006591427606694839\n",
      "\tEpoch 94  train\tAverage Loss:  0.006546433550970895\n",
      "\tEpoch 94  valid\tAverage Loss:  0.008251089278389426\n",
      "\tEpoch 95  train\tAverage Loss:  0.007529478209359305\n",
      "\tEpoch 95  valid\tAverage Loss:  0.007081978180829216\n",
      "\tEpoch 96  train\tAverage Loss:  0.008125250620501382\n",
      "\tEpoch 96  valid\tAverage Loss:  0.00934561666320352\n",
      "\tEpoch 97  train\tAverage Loss:  0.006894900986126491\n",
      "\tEpoch 97  valid\tAverage Loss:  0.007909351096433752\n",
      "\tEpoch 98  train\tAverage Loss:  0.007362433518682207\n",
      "\tEpoch 98  valid\tAverage Loss:  0.0077967453704160805\n",
      "\tEpoch 99  train\tAverage Loss:  0.008180448693888528\n",
      "\tEpoch 99  valid\tAverage Loss:  0.009430962695794947\n",
      "\tEpoch 100  train\tAverage Loss:  0.007450954990727561\n",
      "\tEpoch 100  valid\tAverage Loss:  0.006587036567575791\n",
      "\tEpoch 101  train\tAverage Loss:  0.007864027644906724\n",
      "\tEpoch 101  valid\tAverage Loss:  0.010275262524099911\n",
      "\tEpoch 102  train\tAverage Loss:  0.006990289611475809\n",
      "\tEpoch 102  valid\tAverage Loss:  0.006633865693036247\n",
      "\tEpoch 103  train\tAverage Loss:  0.007691636596407209\n",
      "\tEpoch 103  valid\tAverage Loss:  0.010650922375566819\n",
      "\tEpoch 104  train\tAverage Loss:  0.006963076412677765\n",
      "\tEpoch 104  valid\tAverage Loss:  0.008036466416190652\n",
      "\tEpoch 105  train\tAverage Loss:  0.00685443298305784\n",
      "\tEpoch 105  valid\tAverage Loss:  0.006928210626630222\n",
      "\tEpoch 106  train\tAverage Loss:  0.007435141955103193\n",
      "\tEpoch 106  valid\tAverage Loss:  0.010984605585827548\n",
      "\tEpoch 107  train\tAverage Loss:  0.0073313528214182175\n",
      "\tEpoch 107  valid\tAverage Loss:  0.00854122757911682\n",
      "\tEpoch 108  train\tAverage Loss:  0.006751645267009735\n",
      "\tEpoch 108  valid\tAverage Loss:  0.00779750291038962\n",
      "\tEpoch 109  train\tAverage Loss:  0.007666555864470345\n",
      "\tEpoch 109  valid\tAverage Loss:  0.009581822086783017\n",
      "\tEpoch 110  train\tAverage Loss:  0.008450147262641362\n",
      "\tEpoch 110  valid\tAverage Loss:  0.007017482098411111\n",
      "\tEpoch 111  train\tAverage Loss:  0.00681834306035723\n",
      "\tEpoch 111  valid\tAverage Loss:  0.006599369820426492\n",
      "\tEpoch 112  train\tAverage Loss:  0.00734497982263565\n",
      "\tEpoch 112  valid\tAverage Loss:  0.011001129395821515\n",
      "\tEpoch 113  train\tAverage Loss:  0.007594179034233093\n",
      "\tEpoch 113  valid\tAverage Loss:  0.006940721588976243\n",
      "\tEpoch 114  train\tAverage Loss:  0.007361881562641689\n",
      "\tEpoch 114  valid\tAverage Loss:  0.010924776371787575\n",
      "\tEpoch 115  train\tAverage Loss:  0.007555080728871482\n",
      "\tEpoch 115  valid\tAverage Loss:  0.006820805686361649\n",
      "\tEpoch 116  train\tAverage Loss:  0.00738663033076695\n",
      "\tEpoch 116  valid\tAverage Loss:  0.010607055986628812\n",
      "\tEpoch 117  train\tAverage Loss:  0.007132733958108085\n",
      "\tEpoch 117  valid\tAverage Loss:  0.007951964735984801\n",
      "\tEpoch 118  train\tAverage Loss:  0.007063496266092573\n",
      "\tEpoch 118  valid\tAverage Loss:  0.007192093168987947\n",
      "\tEpoch 119  train\tAverage Loss:  0.008709091229098184\n",
      "\tEpoch 119  valid\tAverage Loss:  0.015573965731789085\n",
      "\tEpoch 120  train\tAverage Loss:  0.009404820084571839\n",
      "\tEpoch 120  valid\tAverage Loss:  0.009218864440917969\n",
      "\tEpoch 121  train\tAverage Loss:  0.007040725690977914\n",
      "\tEpoch 121  valid\tAverage Loss:  0.006658373106928433\n",
      "\tEpoch 122  train\tAverage Loss:  0.007471653606210436\n",
      "\tEpoch 122  valid\tAverage Loss:  0.010303145471741172\n",
      "\tEpoch 123  train\tAverage Loss:  0.006891128667763301\n",
      "\tEpoch 123  valid\tAverage Loss:  0.007400291316649493\n",
      "\tEpoch 124  train\tAverage Loss:  0.007046056841100965\n",
      "\tEpoch 124  valid\tAverage Loss:  0.0066860796072903805\n",
      "\tEpoch 125  train\tAverage Loss:  0.0072202943308012826\n",
      "\tEpoch 125  valid\tAverage Loss:  0.010394647822660558\n",
      "\tEpoch 126  train\tAverage Loss:  0.007120032404150282\n",
      "\tEpoch 126  valid\tAverage Loss:  0.007560920364716474\n",
      "\tEpoch 127  train\tAverage Loss:  0.007014311339173998\n",
      "\tEpoch 127  valid\tAverage Loss:  0.006775581486084882\n",
      "\tEpoch 128  train\tAverage Loss:  0.007264916632856642\n",
      "\tEpoch 128  valid\tAverage Loss:  0.01038847400861628\n",
      "\tEpoch 129  train\tAverage Loss:  0.008285506921155112\n",
      "\tEpoch 129  valid\tAverage Loss:  0.006626535128144657\n",
      "\tEpoch 130  train\tAverage Loss:  0.007671779487814222\n",
      "\tEpoch 130  valid\tAverage Loss:  0.008963653129689834\n",
      "\tEpoch 131  train\tAverage Loss:  0.007015188796179636\n",
      "\tEpoch 131  valid\tAverage Loss:  0.007145087070324842\n",
      "\tEpoch 132  train\tAverage Loss:  0.006614816657134465\n",
      "\tEpoch 132  valid\tAverage Loss:  0.007849215093780967\n",
      "\tEpoch 133  train\tAverage Loss:  0.006690955741064889\n",
      "\tEpoch 133  valid\tAverage Loss:  0.007962678951375625\n",
      "\tEpoch 134  train\tAverage Loss:  0.00673741398538862\n",
      "\tEpoch 134  valid\tAverage Loss:  0.007361679603071773\n",
      "\tEpoch 135  train\tAverage Loss:  0.006958543470927647\n",
      "\tEpoch 135  valid\tAverage Loss:  0.006763447821140289\n",
      "\tEpoch 136  train\tAverage Loss:  0.007245742985180446\n",
      "\tEpoch 136  valid\tAverage Loss:  0.010382027485791374\n",
      "\tEpoch 137  train\tAverage Loss:  0.008246890928064073\n",
      "\tEpoch 137  valid\tAverage Loss:  0.006814952738144819\n",
      "\tEpoch 138  train\tAverage Loss:  0.007089916595390865\n",
      "\tEpoch 138  valid\tAverage Loss:  0.009854019775110133\n",
      "\tEpoch 139  train\tAverage Loss:  0.008138124150889261\n",
      "\tEpoch 139  valid\tAverage Loss:  0.006648038538063274\n",
      "\tEpoch 140  train\tAverage Loss:  0.007544465226786477\n",
      "\tEpoch 140  valid\tAverage Loss:  0.009400236641659456\n",
      "\tEpoch 141  train\tAverage Loss:  0.006883431153638022\n",
      "\tEpoch 141  valid\tAverage Loss:  0.007454503388965831\n",
      "\tEpoch 142  train\tAverage Loss:  0.006845820937837873\n",
      "\tEpoch 142  valid\tAverage Loss:  0.006703971080920275\n",
      "\tEpoch 143  train\tAverage Loss:  0.007113253831863404\n",
      "\tEpoch 143  valid\tAverage Loss:  0.009480180144309997\n",
      "\tEpoch 144  train\tAverage Loss:  0.007994682269436972\n",
      "\tEpoch 144  valid\tAverage Loss:  0.006602276738952188\n",
      "\tEpoch 145  train\tAverage Loss:  0.007452855919088636\n",
      "\tEpoch 145  valid\tAverage Loss:  0.008831020558581633\n",
      "\tEpoch 146  train\tAverage Loss:  0.006831635160105569\n",
      "\tEpoch 146  valid\tAverage Loss:  0.006984257435097414\n",
      "\tEpoch 147  train\tAverage Loss:  0.006673035400254386\n",
      "\tEpoch 147  valid\tAverage Loss:  0.006835542794536142\n",
      "\tEpoch 148  train\tAverage Loss:  0.006781643969672067\n",
      "\tEpoch 148  valid\tAverage Loss:  0.0065975761764189775\n",
      "\tEpoch 149  train\tAverage Loss:  0.007502242514065334\n",
      "\tEpoch 149  valid\tAverage Loss:  0.008920518826035893\n",
      "\tEpoch 150  train\tAverage Loss:  0.007201684474945068\n",
      "\tEpoch 150  valid\tAverage Loss:  0.009508373246473424\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "network.train()\n",
    "\n",
    "print(\"training network...\")\n",
    "for epoch in range(epochs):\n",
    "    train_overall_loss = 0\n",
    "\n",
    "    valid_overall_loss = 0\n",
    "\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        x = x.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        out = network(x)\n",
    "        loss = loss_function(out,y.reshape(-1,1))\n",
    "\n",
    "        train_overall_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "      for valid_batch_idx, (valid_x, valid_y) in enumerate(valid_loader):\n",
    "        valid_x = valid_x.to(device)\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "\n",
    "        valid_out = network(valid_x)\n",
    "        valid_loss = loss_function(valid_out, valid_y.reshape(-1,1))\n",
    "\n",
    "        valid_overall_loss += valid_loss.item()\n",
    "\n",
    "    train_n_datapoints = batch_idx * batch_size\n",
    "    valid_n_datapoints = valid_batch_idx * batch_size\n",
    "\n",
    "    print(\"\\tEpoch\", epoch + 1, \" train\\tAverage Loss: \", train_overall_loss / train_n_datapoints)\n",
    "    print(\"\\tEpoch\", epoch + 1, \" valid\\tAverage Loss: \", valid_overall_loss / valid_n_datapoints)\n",
    "    # # stopping criterion\n",
    "    # if(len(train_elbo)>5):\n",
    "    #   if(train_elbo[-6]-train_elbo[-1]<0.005):\n",
    "    #     break\n",
    "print(\"Training complete!\")\n",
    "# torch.save(vae,\"./model/vae_adam_l40_e100.pt\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vae.eval()\n",
    "\n",
    "x_original_list = []\n",
    "y_list = []\n",
    "x_reconstr_list = []\n",
    "mu_list = []\n",
    "log_sigma_list = []\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x, y) in enumerate(tqdm(test_loader)):\n",
    "        x = x.to(device)\n",
    "\n",
    "        x_reconstr, mu, log_sigma = vae(x)\n",
    "        x_original_list.append(x)\n",
    "        y_list.append(y)\n",
    "        x_reconstr_list.append(x_reconstr)\n",
    "        mu_list.append(mu)\n",
    "        log_sigma_list.append(log_sigma)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(len(train_elbo)),train_elbo,label=\"train\")\n",
    "plt.plot(range(len(valid_elbo)),valid_elbo,label=\"valid\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"train elbo\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
